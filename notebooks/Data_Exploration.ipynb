{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ” KITTI Dataset Exploration\n",
    "\n",
    "Explore the KITTI dataset structure and understand the data format for camera-LiDAR fusion.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The KITTI dataset provides:\n",
    "- ðŸ“· **Stereo camera images** (RGB, 1241x376 pixels)\n",
    "- ðŸŽ¯ **LiDAR point clouds** (Velodyne HDL-64E, ~150K points/frame)\n",
    "- ðŸ“ **Calibration matrices** (Camera-LiDAR transformation)\n",
    "- ðŸ·ï¸ **Ground truth labels** (3D bounding boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "!git clone https://github.com/cclasher916-spec/camera-lidar-fusion.git\n",
    "import os\n",
    "os.chdir('/content/camera-lidar-fusion')\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.data_loader import KITTIDataLoader\n",
    "from src.data.preprocessor import DataPreprocessor\n",
    "\n",
    "print('âœ… Environment setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "config = {\n",
    "    'dataset_path': 'data/kitti',\n",
    "    'sequence': '0001',\n",
    "    'max_samples': 10\n",
    "}\n",
    "\n",
    "data_loader = KITTIDataLoader(config)\n",
    "\n",
    "# Get dataset information\n",
    "info = data_loader.get_sequence_info()\n",
    "print('ðŸ“Š Dataset Information:')\n",
    "for key, value in info.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine a sample\n",
    "sample = data_loader.load_sample(0)\n",
    "\n",
    "print('ðŸ“‹ Sample Structure:')\n",
    "for key, value in sample.items():\n",
    "    if key == 'camera_image':\n",
    "        print(f'  {key}: shape {value.shape}, dtype {value.dtype}')\n",
    "    elif key == 'lidar_points':\n",
    "        print(f'  {key}: shape {value.shape}, {len(value)} points')\n",
    "    elif key == 'calibration':\n",
    "        print(f'  {key}: {len(value)} matrices')\n",
    "        for k in value.keys():\n",
    "            print(f'    - {k}: {value[k].shape}')\n",
    "    elif key == 'ground_truth':\n",
    "        print(f'  {key}: {len(value)} objects')\n",
    "    else:\n",
    "        print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera image\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample['camera_image'])\n",
    "plt.title('ðŸ“· Camera Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Show image histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, color in enumerate(['red', 'green', 'blue']):\n",
    "    hist = cv2.calcHist([sample['camera_image']], [i], None, [256], [0, 256])\n",
    "    plt.plot(hist, color=color, alpha=0.7)\n",
    "plt.title('RGB Histogram')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze LiDAR point cloud\n",
    "points = sample['lidar_points']\n",
    "\n",
    "print('ðŸŽ¯ LiDAR Point Cloud Analysis:')\n",
    "print(f'  Total points: {len(points):,}')\n",
    "print(f'  Point format: [x, y, z, intensity]')\n",
    "print(f'  X range: [{points[:, 0].min():.2f}, {points[:, 0].max():.2f}] meters')\n",
    "print(f'  Y range: [{points[:, 1].min():.2f}, {points[:, 1].max():.2f}] meters')\n",
    "print(f'  Z range: [{points[:, 2].min():.2f}, {points[:, 2].max():.2f}] meters')\n",
    "print(f'  Intensity range: [{points[:, 3].min():.2f}, {points[:, 3].max():.2f}]')\n",
    "\n",
    "# Visualize point cloud statistics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Top-down view\n",
    "ax1.scatter(points[:, 0], points[:, 1], c=points[:, 2], s=0.1, cmap='viridis')\n",
    "ax1.set_xlabel('X (meters)')\n",
    "ax1.set_ylabel('Y (meters)')\n",
    "ax1.set_title('Top-down View (colored by height)')\n",
    "ax1.axis('equal')\n",
    "\n",
    "# Side view\n",
    "ax2.scatter(points[:, 0], points[:, 2], c=points[:, 3], s=0.1, cmap='plasma')\n",
    "ax2.set_xlabel('X (meters)')\n",
    "ax2.set_ylabel('Z (meters)')\n",
    "ax2.set_title('Side View (colored by intensity)')\n",
    "\n",
    "# Distance distribution\n",
    "distances = np.sqrt(points[:, 0]**2 + points[:, 1]**2)\n",
    "ax3.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Distance from sensor (meters)')\n",
    "ax3.set_ylabel('Point count')\n",
    "ax3.set_title('Distance Distribution')\n",
    "\n",
    "# Height distribution\n",
    "ax4.hist(points[:, 2], bins=50, alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Height (meters)')\n",
    "ax4.set_ylabel('Point count')\n",
    "ax4.set_title('Height Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine calibration matrices\n",
    "calibration = sample['calibration']\n",
    "\n",
    "print('ðŸ“ Calibration Matrices:')\n",
    "\n",
    "if 'P2' in calibration:\n",
    "    P2 = calibration['P2']\n",
    "    print('Camera Projection Matrix (P2):')\n",
    "    print(P2)\n",
    "    \n",
    "    # Extract camera parameters\n",
    "    fx, fy = P2[0, 0], P2[1, 1]\n",
    "    cx, cy = P2[0, 2], P2[1, 2]\n",
    "    print(f'\nCamera Parameters:')\n",
    "    print(f'  Focal length: fx={fx:.1f}, fy={fy:.1f}')\n",
    "    print(f'  Principal point: cx={cx:.1f}, cy={cy:.1f}')\n",
    "\n",
    "if 'Tr_velo_to_cam' in calibration:\n",
    "    Tr = calibration['Tr_velo_to_cam']\n",
    "    print('\nLiDAR to Camera Transformation:')\n",
    "    print(Tr)\n",
    "\n",
    "if 'R0_rect' in calibration:\n",
    "    R0 = calibration['R0_rect']\n",
    "    print('\nRectification Matrix:')\n",
    "    print(R0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine ground truth annotations\n",
    "gt = sample['ground_truth']\n",
    "\n",
    "print('ðŸ·ï¸ Ground Truth Annotations:')\n",
    "print(f'  Number of objects: {len(gt)}')\n",
    "\n",
    "for i, obj in enumerate(gt):\n",
    "    print(f'\n  Object {i+1}:')\n",
    "    print(f'    Type: {obj[\"type\"]}')\n",
    "    if 'bbox_2d' in obj:\n",
    "        bbox_2d = obj['bbox_2d']\n",
    "        print(f'    2D BBox: [{bbox_2d[0]:.1f}, {bbox_2d[1]:.1f}, {bbox_2d[2]:.1f}, {bbox_2d[3]:.1f}]')\n",
    "    if 'bbox_3d' in obj:\n",
    "        bbox_3d = obj['bbox_3d']\n",
    "        print(f'    3D Center: [{bbox_3d[0]:.1f}, {bbox_3d[1]:.1f}, {bbox_3d[2]:.1f}]')\n",
    "        if len(bbox_3d) > 3:\n",
    "            print(f'    3D Size: [{bbox_3d[3]:.1f}, {bbox_3d[4]:.1f}, {bbox_3d[5]:.1f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ground truth on image\n",
    "img_with_gt = sample['camera_image'].copy()\n",
    "\n",
    "for obj in gt:\n",
    "    if 'bbox_2d' in obj:\n",
    "        bbox = obj['bbox_2d']\n",
    "        x1, y1, x2, y2 = [int(x) for x in bbox]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img_with_gt, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label = obj['type']\n",
    "        cv2.putText(img_with_gt, label, (x1, y1-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.imshow(img_with_gt)\n",
    "plt.title('ðŸ“· Camera Image with Ground Truth Annotations')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bird's eye view visualization\n",
    "preprocessor_config = {\n",
    "    'lidar_range': {\n",
    "        'x': [-50, 50],\n",
    "        'y': [-25, 25],\n",
    "        'z': [-3, 5]\n",
    "    }\n",
    "}\n",
    "\n",
    "preprocessor = DataPreprocessor(preprocessor_config)\n",
    "bev_image = preprocessor.create_bird_eye_view(points)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(bev_image)\n",
    "plt.title(\"ðŸŽ¯ LiDAR Bird's Eye View\\n(Red: Height, Green: Intensity, Blue: Density)\")\n",
    "plt.xlabel('X direction (forward)')\n",
    "plt.ylabel('Y direction (left)')\n",
    "\n",
    "# Add scale information\n",
    "height, width = bev_image.shape[:2]\n",
    "x_range = preprocessor_config['lidar_range']['x']\n",
    "y_range = preprocessor_config['lidar_range']['y']\n",
    "\n",
    "plt.xticks([0, width//4, width//2, 3*width//4, width-1], \n",
    "          [f'{x:.0f}m' for x in np.linspace(x_range[0], x_range[1], 5)])\n",
    "plt.yticks([0, height//4, height//2, 3*height//4, height-1],\n",
    "          [f'{y:.0f}m' for y in np.linspace(y_range[1], y_range[0], 5)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple samples\n",
    "num_samples = min(4, data_loader.get_sample_count())\n",
    "\n",
    "fig, axes = plt.subplots(2, num_samples, figsize=(20, 8))\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = data_loader.load_sample(i)\n",
    "    \n",
    "    # Camera image\n",
    "    axes[0, i].imshow(sample['camera_image'])\n",
    "    axes[0, i].set_title(f'Sample {i} - Camera')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # LiDAR bird's eye view\n",
    "    bev = preprocessor.create_bird_eye_view(sample['lidar_points'])\n",
    "    axes[1, i].imshow(bev)\n",
    "    axes[1, i].set_title(f'Sample {i} - LiDAR BEV')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('ðŸ“Š Multi-Sample Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing analysis\n",
    "original_points = sample['lidar_points']\n",
    "processed_points = preprocessor.preprocess_lidar(original_points)\n",
    "\n",
    "stats = preprocessor.get_preprocessing_stats(\n",
    "    sample['camera_image'], \n",
    "    original_points\n",
    ")\n",
    "\n",
    "print('ðŸ”„ Preprocessing Statistics:')\n",
    "for key, value in stats.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "# Visualize filtering effects\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(original_points[:, 0], original_points[:, 1], \n",
    "           c=original_points[:, 2], s=0.1, cmap='viridis')\n",
    "plt.title(f'Original Points ({len(original_points):,})')\n",
    "plt.xlabel('X (m)')\n",
    "plt.ylabel('Y (m)')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(processed_points[:, 0], processed_points[:, 1], \n",
    "           c=processed_points[:, 2], s=0.1, cmap='viridis')\n",
    "plt.title(f'Filtered Points ({len(processed_points):,})')\n",
    "plt.xlabel('X (m)')\n",
    "plt.ylabel('Y (m)')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "reduction = (len(original_points) - len(processed_points)) / len(original_points) * 100\n",
    "plt.bar(['Original', 'Filtered'], [len(original_points), len(processed_points)], \n",
    "        color=['blue', 'orange'], alpha=0.7)\n",
    "plt.title(f'Point Reduction: {reduction:.1f}%')\n",
    "plt.ylabel('Number of Points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Summary\n",
    "\n",
    "This exploration revealed:\n",
    "\n",
    "### Data Characteristics\n",
    "- **Camera**: High-resolution RGB images with rich semantic information\n",
    "- **LiDAR**: Dense 3D point clouds with accurate geometric information\n",
    "- **Calibration**: Precise sensor alignment for coordinate transformation\n",
    "\n",
    "### Key Insights\n",
    "1. **Complementary Information**: Camera provides texture/color, LiDAR provides depth\n",
    "2. **Different Failure Modes**: Each sensor has unique strengths and weaknesses\n",
    "3. **Preprocessing Importance**: Filtering reduces noise and computational load\n",
    "4. **Ground Truth Quality**: Accurate annotations enable proper evaluation\n",
    "\n",
    "### Fusion Motivation\n",
    "The exploration confirms that camera and LiDAR sensors provide complementary information that can be fused to create a more robust perception system than either sensor alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}